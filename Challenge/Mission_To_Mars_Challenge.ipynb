{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter, BeautifulSoup, and Pandas\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Driver [/Users/michaelmanthey/.wdm/drivers/chromedriver/mac64/93.0.4577.63/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Set the executable path and initialize Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the mars nasa news site\n",
    "url = 'https://redplanetscience.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Optional delay for loading the page\n",
    "browser.is_element_present_by_css('div.list_text', wait_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the browser html to a soup object and then quit the browser\n",
    "html = browser.html\n",
    "news_soup = soup(html, 'html.parser')\n",
    "\n",
    "slide_elem = news_soup.select_one('div.list_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_elem.find('div', class_='content_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the parent element to find the first a tag and save it as `news_title`\n",
    "news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the parent element to find the paragraph text\n",
    "news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit URL\n",
    "url = 'https://spaceimages-mars.com'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and click the full image button\n",
    "full_image_elem = browser.find_by_tag('button')[1]\n",
    "full_image_elem.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the resulting html with soup\n",
    "html = browser.html\n",
    "img_soup = soup(html, 'html.parser')\n",
    "img_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the relative image url\n",
    "img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "img_url_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the base url to create an absolute url\n",
    "img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['Description', 'Mars', 'Earth']\n",
    "df.set_index('Description', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cerberus.html', 'schiaparelli.html', 'syrtis.html', 'valles.html']\n",
      "https://marshemispheres.com/cerberus.html\n",
      "https://marshemispheres.com/schiaparelli.html\n",
      "https://marshemispheres.com/syrtis.html\n",
      "https://marshemispheres.com/valles.html\n"
     ]
    }
   ],
   "source": [
    "# This cell will grab the image urls\n",
    "\n",
    "#Use browser to visit the URL \n",
    "url = 'https://marshemispheres.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "html = browser.html\n",
    "img_soup = soup(html, 'html.parser')\n",
    "\n",
    "#-- Get hrefs and run for loop to create html list --------#\n",
    "href_results = img_soup('a', class_='itemLink')\n",
    "href_list = []\n",
    "x = 0\n",
    "\n",
    "for x in range(9):\n",
    "    href = href_results[x].get('href')\n",
    "    href_list.append(href)\n",
    "    x =+ 1\n",
    "\n",
    "final_href_list = [href_list[0], href_list[2], href_list[4], href_list[6]]\n",
    "print(final_href_list)\n",
    "\n",
    "#---For Loop creates url and scrapes for jpg image, appends to list--#\n",
    "\n",
    "results_list = []\n",
    "y = 0\n",
    "\n",
    "for y in range(4):\n",
    "    url1 = final_href_list[y]\n",
    "    scrape_url = url + url1\n",
    "    print(scrape_url)\n",
    "    browser.visit(scrape_url)\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "    jpg_result = img_soup.find('img', class_='wide-image').get('src')\n",
    "    results_list.append(jpg_result)\n",
    "    y =+1\n",
    "        \n",
    "#print(results_list)\n",
    "\n",
    "#--- For loop to create final list for jpg urls----------------#\n",
    "final_url_list = []\n",
    "z = 0\n",
    "\n",
    "for z in range(4):\n",
    "    base = 'https://marshemispheres.com/'\n",
    "    mid = final_href_list[z] + \"/\"\n",
    "    end = results_list[z]\n",
    "    final_url = base + end\n",
    "    final_url_list.append(final_url)\n",
    "    z =+ 1\n",
    "    \n",
    "#print(final_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cerberus Hemisphere Enhanced', 'Schiaparelli Hemisphere Enhanced', 'Syrtis Major Hemisphere Enhanced', 'Valles Marineris Hemisphere Enhanced']\n"
     ]
    }
   ],
   "source": [
    "# This Cell will grab the titles of the images\n",
    "\n",
    "url = 'https://marshemispheres.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "html = browser.html\n",
    "img_soup = soup(html, 'html.parser')\n",
    "\n",
    "results_titles = img_soup.find_all('h3')\n",
    "results_titles\n",
    "\n",
    "titles_list = []\n",
    "a = 0\n",
    "\n",
    "for a in range(4):\n",
    "    title = results_titles[a].get_text()\n",
    "    titles_list.append(title)\n",
    "    a =+ 1\n",
    "print(titles_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image url': 'https://marshemispheres.com/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg', 'title': 'Cerberus Hemisphere Enhanced'}, {'image url': 'https://marshemispheres.com/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg', 'title': 'Schiaparelli Hemisphere Enhanced'}, {'image url': 'https://marshemispheres.com/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg', 'title': 'Syrtis Major Hemisphere Enhanced'}, {'image url': 'https://marshemispheres.com/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg', 'title': 'Valles Marineris Hemisphere Enhanced'}]\n"
     ]
    }
   ],
   "source": [
    "# This cell will create the dictionary holding both the image url and image title.\n",
    "cer_hemi = {'image url': final_url_list[0],\n",
    "            'title': titles_list[0],\n",
    "}\n",
    "sch_hemi = {'image url': final_url_list[1],\n",
    "            'title': titles_list[1],\n",
    "}\n",
    "syr_hemi = {'image url': final_url_list[2],\n",
    "            'title': titles_list[2],\n",
    "}\n",
    "val_hemi = {'image url': final_url_list[3],\n",
    "            'title': titles_list[3],\n",
    "}\n",
    "\n",
    "hemisphere_image_urls = [cer_hemi, sch_hemi, syr_hemi, val_hemi]\n",
    "print(hemisphere_image_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Quit the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
